---
title: "Homework-5"
author: "Dr. Purna Gamage"
---

# Problems 

## Problem-1

Problem 5.10 \#12 in Chihara/Hesterberg. 

The data set FishMercury contains mercury levels (parts per million) for 30 fish caught in lakes in Minnesota.
```{r}
FishMercury <- read.csv("FishMercury.csv")
head(FishMercury)
```

(a) Create a histogram or box-plot of the data. What do you observe?
```{r}
par(mfrow = c(1, 2))
hist(FishMercury$Mercury,
     main = "Histogram of Mercury Levels",
     xlab = "Mercury (ppm)")
boxplot(FishMercury$Mercury,
        main = "Boxplot of Mercury Levels",
        ylab = "Mercury (ppm)")
par(mfrow = c(1, 1))
```
> This histogram demonstrates that most fish have very low mercury levels, less than 0.5 ppm. There is also an outlier at the 1.5-2.0 ppm level, which will have an outside effect on the overall distribution.

(b) Find the Bootstrap sampling mean and record the bootstrap standard error and the $95\%$ bootstrap percentile interval.
```{r}
set.seed(5100)
x <- FishMercury$Mercury
B <- 10000

boot_means <- replicate(B, {
  mean(sample(x, replace = TRUE))
})

boot_se <- sd(boot_means)
boot_ci <- quantile(boot_means, c(0.025, 0.975))

boot_se
boot_ci
mean(x)
```
(c) Remove the outlier and find bootstrap sampling mean of the remaining data. Record the bootstrap standard error and the $95\%$ bootstrap percentile interval. Comment on your results.
```{r}
# Identify the outliers using a box plot
bp <- boxplot(x, plot = FALSE)
bp$out

# Remove outlier
x_no_out <- x[!x %in% bp$out]

length(x)
length(x_no_out)

# Bootstrap on data without outlier
set.seed(5100)
boot_means_no <- replicate(B, {
  mean(sample(x_no_out, replace = TRUE))
})

boot_se_no <- sd(boot_means_no)
boot_ci_no <- quantile(boot_means_no, c(0.025, 0.975))

boot_se_no
boot_ci_no
mean(x_no_out)
```
> Once we remove the outlier, the sample mean and the standard error decreases substantially. This makes sense due to the outlier being so far away from the majority of the dataset.

(d) What effect did removing the outlier have on the bootstrap distribution, in particular, the standard error?
> Removing the outlier significantly changes the distribution. With the outlier removed, the distribution is concentrated around the main cluster of observations, which is the 0.0-0.5 ppm level. This causes the Standard Error to decrease substantially.

## Problem-2 

Problem 3.9 \#12abc in Chihara/Hesterberg. 

Two students went to a local supermarket and collected data on cereals; they classified cereals by their target consumer (children versus adults) and the placement of the cereal on the shelf (bottom, middle, and top). The data are given in _Cereals_.

(a) Create a table (Two-way) to summarize the relationship between age of target consumer and shelf location.
```{r}
Cereals <- read.csv("Cereals.csv")
# Two-way table
table(Cereals$Age, Cereals$Shelf)
```

(b) Conduct a chi-square test using R’s chisq.test() command. Write your null and alternative hypthesis. What is your conclusion based on the results of your test?
```{r}
tab <- table(Cereals$Age, Cereals$Shelf)
tab

chi_res <- chisq.test(tab)
chi_res
```
> Ho: There is no association between target consumer age group and shelf placement.
>
> Ha: There is an association between target consumer age group and shelf palcement.
>
> Conclusion: Because the p-value is less than 0.05, we reject the Null Hypothesis that there is no association between target consumer age group and shelf placement.

(c) R returns a warning message. Compute the expected counts for each cell to see why.
```{r}
tab <- table(Cereals$Age, Cereals$Shelf)

chi_res <- chisq.test(tab)

chi_res$expected
```

<!-- (d) (self-learn question) Use a Fisher's Exact Test. What is your conclusion?. (We use a Fisher's Exact Test when the sample sizes are small and the expected cell counts are less than 5 : Example to refer:<https://statsandr.com/blog/fisher-s-exact-test-in-r-independence-test-for-a-small-sample/>).   -->

<!-- (e) Compare your results  of part (b),(d),(f). Explain/Compare in few sentences where/when/what situations should we use Yate's Continuity correction and Fisher's exact test. -->

<!-- (d) Use a Yate's continuity correction and do the test again. What is your conclusion? -->

## Problem-3

This problem is similar to what we have done in the lab using the spotify data. Please use the "Artists.csv" data set.

Data Science Question: Does the average "liveness" is larger for Beyoncé than that of Taylor Swift?

Liveness: This value describes the probability that the song was recorded with a live audience. According to the official documentation “a value above 0.8 provides strong likelihood that the track is live”.

a. Perform meaningful EDA (Exploratory Data Analysis) using some Data Visualizations (relevant to this data science question).
```{r}
Artists <- read.csv("Artists.csv")

# subset dataset to just have Beyonce and Taylor Swift
sub_art <- subset(Artists, artist_name %in% c("Beyoncé", "Taylor Swift"))

# Box Plot
library(ggplot2)
ggplot(sub_art, aes(x = artist_name, y = liveness, fill = artist_name)) +
  geom_boxplot(alpha = 0.7) +
  theme_minimal() +
  theme(legend.position = "none") +
  labs(title = "Liveness Distribution: Beyoncé vs Taylor Swift",
       x = "Artist",
       y = "Liveness")

# Density Plot
ggplot(sub_art, aes(x = liveness, color = artist_name, fill = artist_name)) +
  geom_density(alpha = 0.3) +
  theme_minimal() +
  labs(title = "Density Plot of Liveness",
       x = "Liveness",
       y = "Density")

# Summary Statistics
tapply(sub_art$liveness, sub_art$artist_name, summary)
```
b. Write the null and alternative hypothesis for this test.
> Ho: Beyonce does not have higher average liveness.
>
> Ha: Beyonce has higher average liveness.

c. Perform a t-test and state your results and non-technical conclusion.
```{r}
t_res <- t.test(liveness ~ artist_name,
                data = sub_art,
                alternative = "greater")
t_res
```
> Beyonce has a higher average liveness than Taylor Swift.

d. What can you say about the confidence interval? (Interpret)

> The 95% one-sided confidence interval for the difference in mean liveness for Beyoncé to Taylor Swift is (0.1026, infiniti). This means that with 95% confidence Beyoncé’s songs are on average at least 0.10 higher in liveness than Taylor Swift’s.

e. Perform a bootstrap test for ratio of means of "liveness", Find the 95% bootstrap percentile interval for the ratio of means and write your conclusion. 
```{r}
set.seed(1000)
bey_liv <- sub_art$liveness[sub_art$artist_name == "Beyoncé"]
tay_liv <- sub_art$liveness[sub_art$artist_name == "Taylor Swift"]
# Number of bootstrap samples
B <- 10000
boot_ratio <- replicate(B, {
  mean(sample(bey_liv, replace = TRUE)) /
    mean(sample(tay_liv, replace = TRUE))
})
boot_ci_ratio <- quantile(boot_ratio, c(0.025, 0.975))
boot_ci_ratio
```
> The ratio of mean liveness for Beyonce compared to Taylor Swift is approximately 1.73, meaning Beyoncé’s average liveness is about 73% higher.

f. What is the bootstrap estimate of the bias for the mean ratio?
```{r}
# Observed ratio of means
obs_ratio <- mean(bey_liv) / mean(tay_liv)

# Bootstrap mean ratio
boot_mean_ratio <- mean(boot_ratio)

# Bootstrap bias estimate
bias_est <- boot_mean_ratio - obs_ratio
bias_est
```

g. Compare your results from part c) and part e).

> In part c, the t-test difference in means show that Beyonce has a higher average liveness than Taylor Swift. The 95% one-sided confidence interval for MBeyonce-MTaylor was (0.1026, infiniti), meaning that we are very confident that Beyonces songs have higher mean liveness by about ten units.

> In part e, the boot-strap analysis of the ratio of means provided a 95 percentile interval of (1.60, 1.87), indicating that Beyonce's liveness is 1.6 to 1.87 times higher than Taylor Swift's.

## Problem-4

Write an R function that computes the t-formula confidence interval in (7.8) from sample mean, sample standard deviation, sample size, and confidence level, and use it to do exercise 7.6 #6 in Chihara/Hesterberg.

Q: Julie is interested in the sugar content of vanilla ice cream. She obtains a random sample of $n = 20$ brands and finds an average of $18.05 g$ with standard deviation $5 g$ (per half cup serving). Assuming that the data come from a normal distribution, find a $90\%$ confidence interval for the mean amount of sugar in a half cup serving of vanilla ice cream.
```{r}
t_ci <- function(xbar, s, n, conf = 0.95) {
  alpha <- 1 - conf
  df <- n - 1
  t_star <- qt(1 - alpha/2, df)
  half_width <- t_star * s / sqrt(n)
  
  lower <- xbar - half_width
  upper <- xbar + half_width
  
  return(c(lower = lower, upper = upper))
}
t_ci(xbar = 18.05, s = 5, n = 20, conf = 0.90)
```
> Interpretation: We are 90% confident that the true average amount of sugar in a half-cup serving of vanilla ice cream falls between approximately 16.1 g and 20.0 g.

# Bonus Problems: 

THE FOLLOWING IS AN OPTIONAL (NOT MANDATORY) BONUS ASSIGNMENT

FOR UP TO +12 BONUS POINTS

## Ex. Problem-0

_Distribution A is a standard normal distribution and distribution B is a $N(1, 2^2)$ distribution. Generate 20 random numbers from distribution A and 30 random numbers form distribution B and  record these in a suitable data frame._

Examine the  null hypothesis that the means of A and B are the same against the alternative that the mean of B is larger, using a permutation test. Report the p-value and state your conclusion.
```{r}
# Create the samples
set.seed(1000)
A <- rnorm(20, mean = 0, sd = 1)
B <- rnorm(30, mean = 1, sd = 2)
# Put in a data frame
df <- data.frame(
  value = c(A, B),
  group = c(rep("A", 20), rep("B", 30))
)
# find difference in means
obs_diff <- mean(B) - mean(A)
obs_diff

# permutation test
B_perm <- 10000
perm_diffs <- replicate(B_perm, {
  shuffled <- sample(df$group)
  mean(df$value[shuffled == "B"]) - mean(df$value[shuffled == "A"])
})
# compute t-test
p_value <- mean(perm_diffs >= obs_diff)
p_value
```
> Interpretation: The permutation test produced a p-value of 0.0023, which is below 0.05. Therefore, we reject the null hypothesis that the two distributions have the same mean. There is strong evidence that the mean of distribution B is larger than the mean of distribution A.

## Ex. Problem-1  

Exercise 7.6 #12 in Chihara/Hasterberg.

Q: Consider the data set _Girls2004_ (see Case Study in Section 1.2).

(a) Create exploratory plots and compare the distribution of weights between babies born to nonsmokers and babies born to smokers.
```{r}
Girls2004 <- read.csv("Girls2004.csv")

# Box Plot - Weight vs Smoker status
ggplot(Girls2004, aes(x = Smoker, y = Weight, fill = Smoker)) +
  geom_boxplot(alpha = 0.7) +
  theme_minimal() +
  labs(title = "Birth Weights by Mother's Smoking Status",
       x = "Mother Smoked During Pregnancy",
       y = "Baby Weight in grams") +
  theme(legend.position = "none")

# Density plot
ggplot(Girls2004, aes(x = Weight, color = Smoker, fill = Smoker)) +
  geom_density(alpha = 0.3) +
  theme_minimal() +
  labs(title = "Density of Birth Weights: Smokers vs Nonsmokers",
       x = "Baby Weight in grams",
       y = "Density")
```

(b) Find a $95\%$ one-sided lower t confidence bound for the mean difference in weights between babies born to nonsmokers and smokers. Give a sentence interpreting the interval.
```{r}
# create separate data sets
nonsmoke <- Girls2004$Weight[Girls2004$Smoker == "No"]
smoke    <- Girls2004$Weight[Girls2004$Smoker == "Yes"]

# compute sample means
mean_non  <- mean(nonsmoke)
mean_smok <- mean(smoke)
diff_mean <- mean_non - mean_smok
diff_mean

# compute standard errors
s_non  <- sd(nonsmoke)
s_smok <- sd(smoke)
n_non  <- length(nonsmoke)
n_smok <- length(smoke)
SE <- sqrt( (s_non^2 / n_non) + (s_smok^2 / n_smok) )
SE

# compute welch df
df <- ( (s_non^2 / n_non) + (s_smok^2 / n_smok) )^2 /
      ( ((s_non^2 / n_non)^2) / (n_non - 1) +
        ((s_smok^2 / n_smok)^2) / (n_smok - 1) )
df

# compute one sided 95% confidence bound
t_star <- qt(0.95, df)
lower_bound <- diff_mean - t_star * SE
lower_bound
```
> The 95% one-sided lower confidence bound for the difference in mean birth weights for non-smoker to smoker is approximately 15 grams. This means we are 95% confident that babies born to nonsmoking mothers weigh at least 15 grams more on average than babies born to smoking mothers.

(c) What is your conclusion?

> Mothers who do not smoke during pregnancy have babies that are on average 15 grams heavier than babies that are born to mothers who smoke during pregnancy.

## Ex. Problem-2  

Exercise 6.4 #1 in Chihara/Hesterberg.

Let X be a binomial random variable, $X \sim Binom(n, p)$. Show that the MLE of $p$ is $\hat{p}  = X/n$.

## Ex. Problem-3

Exercise 6.4 #14 in Chihara/Hesterberg. 

Let the five numbers 2, 3, 5, 9, 10 come from the uniform distribution on $[\alpha, \beta]$. Find the method of moments estimates of $\alpha$ and $\beta$.

